{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/trevorle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/trevorle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports here, no need for structure\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>47</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE</td>\n",
       "      <td>317</td>\n",
       "      <td>l6uf6d</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>53</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Hedgefund whales are spreading disinfo saying ...</td>\n",
       "      <td>2021-01-28 21:26:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>THIS IS THE MOMENT</td>\n",
       "      <td>405</td>\n",
       "      <td>l6ub9l</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>178</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Life isn't fair. My mother always told me that...</td>\n",
       "      <td>2021-01-28 21:19:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We need to keep this movement going, we all ca...</td>\n",
       "      <td>222</td>\n",
       "      <td>l6uao1</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>70</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>I believe right now is one of those rare oppo...</td>\n",
       "      <td>2021-01-28 21:18:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Once you're done with GME - $AG and $SLV, the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>l6u9wu</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>16</td>\n",
       "      <td>1.611861e+09</td>\n",
       "      <td>You guys are champs. GME... who would have tho...</td>\n",
       "      <td>2021-01-28 21:17:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  score      id  \\\n",
       "2                                     Exit the system      0  l6uhhn   \n",
       "6         SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE    317  l6uf6d   \n",
       "7                                  THIS IS THE MOMENT    405  l6ub9l   \n",
       "10  We need to keep this movement going, we all ca...    222  l6uao1   \n",
       "12  Once you're done with GME - $AG and $SLV, the ...      0  l6u9wu   \n",
       "\n",
       "                                                  url  comms_num  \\\n",
       "2   https://www.reddit.com/r/wallstreetbets/commen...         47   \n",
       "6   https://www.reddit.com/r/wallstreetbets/commen...         53   \n",
       "7   https://www.reddit.com/r/wallstreetbets/commen...        178   \n",
       "10  https://www.reddit.com/r/wallstreetbets/commen...         70   \n",
       "12  https://www.reddit.com/r/wallstreetbets/commen...         16   \n",
       "\n",
       "         created                                               body  \\\n",
       "2   1.611862e+09  The CEO of NASDAQ pushed to halt trading “to g...   \n",
       "6   1.611862e+09  Hedgefund whales are spreading disinfo saying ...   \n",
       "7   1.611862e+09  Life isn't fair. My mother always told me that...   \n",
       "10  1.611862e+09   I believe right now is one of those rare oppo...   \n",
       "12  1.611861e+09  You guys are champs. GME... who would have tho...   \n",
       "\n",
       "              timestamp  \n",
       "2   2021-01-28 21:30:35  \n",
       "6   2021-01-28 21:26:27  \n",
       "7   2021-01-28 21:19:31  \n",
       "10  2021-01-28 21:18:25  \n",
       "12  2021-01-28 21:17:10  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pull in the data \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('reddit_wsb.csv')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>tokenized_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exit the system</td>\n",
       "      <td>0</td>\n",
       "      <td>l6uhhn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>47</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
       "      <td>2021-01-28 21:30:35</td>\n",
       "      <td>[Exit, the, system]</td>\n",
       "      <td>[The CEO of NASDAQ pushed to halt trading “to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE</td>\n",
       "      <td>317</td>\n",
       "      <td>l6uf6d</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>53</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Hedgefund whales are spreading disinfo saying ...</td>\n",
       "      <td>2021-01-28 21:26:27</td>\n",
       "      <td>[SHORT, STOCK, DOES, N'T, HAVE, AN, EXPIRATION...</td>\n",
       "      <td>[Hedgefund whales are spreading disinfo saying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>THIS IS THE MOMENT</td>\n",
       "      <td>405</td>\n",
       "      <td>l6ub9l</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>178</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>Life isn't fair. My mother always told me that...</td>\n",
       "      <td>2021-01-28 21:19:31</td>\n",
       "      <td>[THIS, IS, THE, MOMENT]</td>\n",
       "      <td>[Life isn't fair., My mother always told me th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We need to keep this movement going, we all ca...</td>\n",
       "      <td>222</td>\n",
       "      <td>l6uao1</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>70</td>\n",
       "      <td>1.611862e+09</td>\n",
       "      <td>I believe right now is one of those rare oppo...</td>\n",
       "      <td>2021-01-28 21:18:25</td>\n",
       "      <td>[We, need, to, keep, this, movement, going, ,,...</td>\n",
       "      <td>[ I believe right now is one of those rare opp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Once you're done with GME - $AG and $SLV, the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>l6u9wu</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>16</td>\n",
       "      <td>1.611861e+09</td>\n",
       "      <td>You guys are champs. GME... who would have tho...</td>\n",
       "      <td>2021-01-28 21:17:10</td>\n",
       "      <td>[Once, you, 're, done, with, GME, -, $, AG, an...</td>\n",
       "      <td>[You guys are champs., GME... who would have t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53181</th>\n",
       "      <td>Ten Year Price Prediction for TSLA</td>\n",
       "      <td>156</td>\n",
       "      <td>owfbxp</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>204</td>\n",
       "      <td>1.627913e+09</td>\n",
       "      <td>It’s all contingent on them mastering FSD, but...</td>\n",
       "      <td>2021-08-02 17:11:36</td>\n",
       "      <td>[Ten, Year, Price, Prediction, for, TSLA]</td>\n",
       "      <td>[It’s all contingent on them mastering FSD, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53182</th>\n",
       "      <td>What I Learned Investigating SAVA FUD Spreaders</td>\n",
       "      <td>238</td>\n",
       "      <td>owd2pn</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>87</td>\n",
       "      <td>1.627906e+09</td>\n",
       "      <td>***TLDR: Three bitter scientists partnered up ...</td>\n",
       "      <td>2021-08-02 15:03:27</td>\n",
       "      <td>[What, I, Learned, Investigating, SAVA, FUD, S...</td>\n",
       "      <td>[***TLDR: Three bitter scientists partnered up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53183</th>\n",
       "      <td>Daily Popular Tickers Thread for August 02, 20...</td>\n",
       "      <td>228</td>\n",
       "      <td>owd1a5</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>1070</td>\n",
       "      <td>1.627906e+09</td>\n",
       "      <td>\\nYour daily hype thread. Please keep the shit...</td>\n",
       "      <td>2021-08-02 15:01:03</td>\n",
       "      <td>[Daily, Popular, Tickers, Thread, for, August,...</td>\n",
       "      <td>[\\nYour daily hype thread., Please keep the sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53185</th>\n",
       "      <td>Daily Discussion Thread for August 02, 2021</td>\n",
       "      <td>338</td>\n",
       "      <td>owbfjf</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>11688</td>\n",
       "      <td>1.627898e+09</td>\n",
       "      <td>Your daily trading discussion thread. Please k...</td>\n",
       "      <td>2021-08-02 13:00:16</td>\n",
       "      <td>[Daily, Discussion, Thread, for, August, 02, ,...</td>\n",
       "      <td>[Your daily trading discussion thread., Please...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53186</th>\n",
       "      <td>Fraternal Association of Gambling Gentlemen an...</td>\n",
       "      <td>40</td>\n",
       "      <td>owaqd6</td>\n",
       "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>810</td>\n",
       "      <td>1.627895e+09</td>\n",
       "      <td>This is an old Yacht Club thread. Click /u/Vis...</td>\n",
       "      <td>2021-08-02 12:00:14</td>\n",
       "      <td>[Fraternal, Association, of, Gambling, Gentlem...</td>\n",
       "      <td>[This is an old Yacht Club thread., Click /u/V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24738 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  score      id  \\\n",
       "2                                        Exit the system      0  l6uhhn   \n",
       "6            SHORT STOCK DOESN'T HAVE AN EXPIRATION DATE    317  l6uf6d   \n",
       "7                                     THIS IS THE MOMENT    405  l6ub9l   \n",
       "10     We need to keep this movement going, we all ca...    222  l6uao1   \n",
       "12     Once you're done with GME - $AG and $SLV, the ...      0  l6u9wu   \n",
       "...                                                  ...    ...     ...   \n",
       "53181                 Ten Year Price Prediction for TSLA    156  owfbxp   \n",
       "53182    What I Learned Investigating SAVA FUD Spreaders    238  owd2pn   \n",
       "53183  Daily Popular Tickers Thread for August 02, 20...    228  owd1a5   \n",
       "53185        Daily Discussion Thread for August 02, 2021    338  owbfjf   \n",
       "53186  Fraternal Association of Gambling Gentlemen an...     40  owaqd6   \n",
       "\n",
       "                                                     url  comms_num  \\\n",
       "2      https://www.reddit.com/r/wallstreetbets/commen...         47   \n",
       "6      https://www.reddit.com/r/wallstreetbets/commen...         53   \n",
       "7      https://www.reddit.com/r/wallstreetbets/commen...        178   \n",
       "10     https://www.reddit.com/r/wallstreetbets/commen...         70   \n",
       "12     https://www.reddit.com/r/wallstreetbets/commen...         16   \n",
       "...                                                  ...        ...   \n",
       "53181  https://www.reddit.com/r/wallstreetbets/commen...        204   \n",
       "53182  https://www.reddit.com/r/wallstreetbets/commen...         87   \n",
       "53183  https://www.reddit.com/r/wallstreetbets/commen...       1070   \n",
       "53185  https://www.reddit.com/r/wallstreetbets/commen...      11688   \n",
       "53186  https://www.reddit.com/r/wallstreetbets/commen...        810   \n",
       "\n",
       "            created                                               body  \\\n",
       "2      1.611862e+09  The CEO of NASDAQ pushed to halt trading “to g...   \n",
       "6      1.611862e+09  Hedgefund whales are spreading disinfo saying ...   \n",
       "7      1.611862e+09  Life isn't fair. My mother always told me that...   \n",
       "10     1.611862e+09   I believe right now is one of those rare oppo...   \n",
       "12     1.611861e+09  You guys are champs. GME... who would have tho...   \n",
       "...             ...                                                ...   \n",
       "53181  1.627913e+09  It’s all contingent on them mastering FSD, but...   \n",
       "53182  1.627906e+09  ***TLDR: Three bitter scientists partnered up ...   \n",
       "53183  1.627906e+09  \\nYour daily hype thread. Please keep the shit...   \n",
       "53185  1.627898e+09  Your daily trading discussion thread. Please k...   \n",
       "53186  1.627895e+09  This is an old Yacht Club thread. Click /u/Vis...   \n",
       "\n",
       "                 timestamp                                    tokenized_title  \\\n",
       "2      2021-01-28 21:30:35                                [Exit, the, system]   \n",
       "6      2021-01-28 21:26:27  [SHORT, STOCK, DOES, N'T, HAVE, AN, EXPIRATION...   \n",
       "7      2021-01-28 21:19:31                            [THIS, IS, THE, MOMENT]   \n",
       "10     2021-01-28 21:18:25  [We, need, to, keep, this, movement, going, ,,...   \n",
       "12     2021-01-28 21:17:10  [Once, you, 're, done, with, GME, -, $, AG, an...   \n",
       "...                    ...                                                ...   \n",
       "53181  2021-08-02 17:11:36          [Ten, Year, Price, Prediction, for, TSLA]   \n",
       "53182  2021-08-02 15:03:27  [What, I, Learned, Investigating, SAVA, FUD, S...   \n",
       "53183  2021-08-02 15:01:03  [Daily, Popular, Tickers, Thread, for, August,...   \n",
       "53185  2021-08-02 13:00:16  [Daily, Discussion, Thread, for, August, 02, ,...   \n",
       "53186  2021-08-02 12:00:14  [Fraternal, Association, of, Gambling, Gentlem...   \n",
       "\n",
       "                                          tokenized_body  \n",
       "2      [The CEO of NASDAQ pushed to halt trading “to ...  \n",
       "6      [Hedgefund whales are spreading disinfo saying...  \n",
       "7      [Life isn't fair., My mother always told me th...  \n",
       "10     [ I believe right now is one of those rare opp...  \n",
       "12     [You guys are champs., GME... who would have t...  \n",
       "...                                                  ...  \n",
       "53181  [It’s all contingent on them mastering FSD, bu...  \n",
       "53182  [***TLDR: Three bitter scientists partnered up...  \n",
       "53183  [\\nYour daily hype thread., Please keep the sh...  \n",
       "53185  [Your daily trading discussion thread., Please...  \n",
       "53186  [This is an old Yacht Club thread., Click /u/V...  \n",
       "\n",
       "[24738 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "def tokenize_title(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def tokenize_body(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "df['tokenized_title'] = df['title'].apply(tokenize_title)\n",
    "\n",
    "df['tokenized_body'] = df['body'].apply(tokenize_body)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/trevorle/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/trevorle/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#3rd apply lemmatizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4759\u001b[0m         func,\n\u001b[1;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#3rd apply lemmatizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#3rd apply lemmatizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[0;32m---> 16\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m wn\u001b[38;5;241m.\u001b[39m_morphy(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/trevorle/nltk_data'\n    - '/opt/homebrew/anaconda3/nltk_data'\n    - '/opt/homebrew/anaconda3/share/nltk_data'\n    - '/opt/homebrew/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#normalization\n",
    "#1st apply everything into lower case\n",
    "df['title_lower'] = df['tokenized_title'].apply(lambda x: [token.lower() for token in x])\n",
    "\n",
    "df['body_lower'] = df['tokenized_body'].apply(lambda x: [token.lower() for token in x])\n",
    "\n",
    "#2nd remove all of the stop words.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['title_no_stopwords'] = df['tokenized_title'].apply(lambda x: [token for token in x if token.lower() not in stop_words])\n",
    "\n",
    "df['body_no_stopwords'] = df['tokenized_body'].apply(lambda x: [token for token in x if token.lower() not in stop_words])\n",
    "\n",
    "#3rd apply lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['title_lemmatized'] = df['tokenized_title'].apply(lambda x: [lemmatizer.lemmatize(token) for token in x])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
